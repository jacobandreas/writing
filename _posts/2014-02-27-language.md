---
title: Working notes on language and the representation of meaning
layout: essay
categories: [mind]
---

<link rel='stylesheet' href='../css/bullets.css' />

Presented with apologies.

1. All meaning is use.

   1. Put another way: there is nothing real between syntax and pragmatics.
      Attempts to axiomatize the in-between place miss the point: this is not
      where our reasoning needs to be sound.

   1. Which is not to say there aren't useful abstractions at this level, or
      models with latent variables that correspond to "semantics" in the
      classical sense---only that a map from strings to database queries has
      not yet reached "meaning".

   1. Which, in turn, is not to say that the problem is with the database
      itself. Certainly we need some level of abstraction above individual pixel
      intensities---this is what perception does for us. But in telling me that
      some subset of all utterances correspond to logical forms, you haven't
      yet explained how I will interpret any utterance in this subset, nor
      explained what to do with all the utterances outside of it.

   1. Many standard problems in the philosophy of language ("the morning star
      is the evening star", etc.) arise only because we insist on ascribing
      to sentences meanings independent of their speakers' and hearers' mental
      states.

   1. Grice's timeless meanings are useful, but they are not what we ultimately
      want to model.

   1. I regard the early chapters of _Philosophical Investigations_ as basically
      persuasive on this view.

   1. Or, from a different angle, a colorful anecdote: One of my old professors
      is at a 
      dinner party with Elizabeth Anscombe, who says "Look---I will prove to you
      that a speaker-independent sentence meaning cannot be built up
      compositionally from its parts." As they are leaving the party, she goes
      to the host, shakes his hand, and says "Fuck you very much for dinner."
      The host replies "You're very welcome." 

   1. She means to express thanks, she chooses a set of words which in other
      contexts would convey exactly the opposite, but her thanks are received
      and correctly interpreted. This is only possible if G.E.M.A. has a _very_
      precise model of how the speaker will respond to any set of sounds she
      produces (including corrections by his internal language model). And this
      is _all_ she needs! No reason to worry about how to say "fuck you" in the
      fluent calculus. Can we formalize this?

1. Q: If logical propositions are not enough, what abstraction do we
   use for the representation of meaning? A: A decision process.

   1. More specifically: think of the world you live in as approximated by a
      POMDP. Language forms both a class of observations (when produced by
      others) and a class of actions (when produced by you).

   1. Qua observation in a POMDP, all speech tends to alter the listener's belief
      state.

   1. With the exception of performatives, the _only_ first-order effect language
      can have on the world is to alter belief states.
   
   1. Of course, nobody would ever communicate if it didn't have second-order
      effects: altering others' behavior by altering their belief states.

   1. Language cannot be understood without a model of the speaker's belief
      state and value function. Language cannot be produced without a model of
      the listener's belief state and value function.

   1. This is not quite a private language argument: even if we don't want to
      talk about public meanings, communication is only possible if there is a
      shared inventory of symbols---an equilibrium of a coordination game.

   1. If you want a compact description of an utterance's meaning, relativized
      to an individual speaker, use this: the update that will be made to their
      belief state when it is communicated to them. (This belief state has
      memory, and includes a distribution over all histories consistent with the
      present.)

   1. In a very simple (discrete) state space, this looks like a draw from a
      Dirichlet distribution.

1. Currently, most interesting semantics work looks like transforming questions
   into database queries. How do we build a question answering system that
   bottoms out in a POMDP?

   1. A zeroth-order question answering system receives an utterance, transforms
      it into a database query, executes the query, and prints the result, all
      because it is programmed to do so.

   1. A first-order question answering system receives an utterance, and knows
      that it will receive a reward for returning the output of the query
      corresponding to the utterance.

   1. A second-order question answering system receives an utterance from a
      user, and knows that it will receive a reward for altering that user's
      mind to include a particular true belief associated with the utterance.
      This is what we really want.

1. I can walk up to a stranger on the street, ask for the time of day, and be
   reasonably assured of receiving a correct answer.

   1. What do I mean by "ask for the time of day"? Produce a set of sounds that,
      given my model of the listener (I have a strong prior on the behavior of
      strangers on the street), will produce an appropriate update to his belief
      about my mental state.

   1. This is not the same as "produce a set of sounds that, in another speaker,
      I would interpret as the speaker's attempt to update my belief about his
      mental state". I might be in a foreign country, and have memorized the
      appropriate sequence of muscle movements, but be unable to recognize them
      when spoken to me. Symmetry is not necessary.

   1. What are my assumptions about the stranger? Easier to ask under what
      conditions this procedure fails (i.e. in which I wind up with a false
      belief about the time, or no belief at all). Possibilities: if the
      stranger doesn't the language I use, if the stranger is a sociopath, or if
      the stranger's watch is broken (nonexistent). The first two are
      interesting.

   1. How much language behavior depends on people being mostly altruistic most
      of the time? (Possibly in a stronger sense than Gricean.)

1. The interaction between language and a "database" model of the world is
   complicated.

   1. This world is basically a collection of discrete entities, each with a
      set of properties (inc. part--whole relationships, etc.).

   1. Color is one of these properties.

   1. But we know color is slightly Whorfian---my willingness to classify
      objects as _orange_ or _brown_, or _blue_ or _green_, is at least partly
      a function of my native language.

   1. Similarly, I do not notice parts of some things until they are given names
      distinct from their wholes.

   1. So the schema of this database is certainly not fixed ahead of time.

   1. But structure doesn't only flow from language to schema---also in the
      other direction. Sometimes I notice a new regularity in the world, or a
      new part. In order to incorporate it into my interior monologue, I have to
      give it a name. I do this reflexively.
