---
title: Working notes on language and the representation of meaning
layout: essay
categories: [nlp,mind]
---

<link rel='stylesheet' href='../css/bullets.css' />

Presented with apologies.

1. All meaning is use.

   1. Put another way: there is nothing real between syntax and pragmatics.
      Attempts to axiomatize the in-between place miss the point: this is not
      where our reasoning needs to be sound.

   1. Which is not to say there aren't useful abstractions at this level, or
      models with latent variables that correspond to "semantics" in the
      classical sense---only that a map from strings to database queries has
      not yet reached "meaning".

   1. Which, in turn, is not to say that the problem is with the database
      itself. Certainly we need some level of abstraction above individual pixel
      intensities---this is what perception does for us. But in telling me that
      some subset of all utterances correspond to logical forms, you haven't
      yet explained how I will interpret any utterance in this subset, nor
      explained what to do with all the utterances outside of it.

   1. Many standard problems in the philosophy of language ("the morning star
      is the evening star", etc.) arise only because we insist on ascribing
      to sentences meanings independent of their speakers' and hearers' mental
      states.

   1. Grice's timeless meanings are useful, but they are not what we ultimately
      want to model.

   1. I regard the early chapters of _Philosophical Investigations_ as basically
      persuasive on this view.

   1. Or, from a different angle, a colorful anecdote: One of my old professors
      is at a 
      dinner party with Elizabeth Anscombe, who says "Look---I will prove to you
      that a speaker-independent sentence meaning cannot be built up
      compositionally from its parts." As they are leaving the party, she goes
      to the host, shakes his hand, and says "Fuck you very much for dinner."
      The host replies "You're very welcome." 

   1. She means to express thanks, she chooses a set of words which in other
      contexts would convey exactly the opposite, but her thanks are received
      and correctly interpreted. This is only possible if G.E.M.A. has a _very_
      precise model of how the speaker will respond to any set of sounds she
      produces (including corrections by his internal language model). And this
      is _all_ she needs! No reason to worry about how to say "fuck you" in the
      fluent calculus. Can we formalize this?

1. Q: If logical propositions are not enough, what abstraction do we
   use for the representation of meaning? A: A decision process.

   1. More specifically: think of the world you live in as approximated by a
      POMDP. Language forms both a class of observations (when produced by
      others) and a class of actions (when produced by you).

   1. Qua observation in a POMDP, all speech tends to alter the listener's belief
      state.

   1. With the exception of performatives, the _only_ first-order effect language
      can have on the world is to alter belief states.
   
   1. Of course, nobody would ever communicate if it didn't have second-order
      effects: altering others' behavior by altering their belief states.

   1. Language cannot be understood without a model of the speaker's belief
      state and value function. Language cannot be produced without a model of
      the listener's belief state and value function.

   1. This is not quite a private language argument: even if we don't want to
      talk about public meanings, communication is only possible if there is a
      shared inventory of symbols---an equilibrium of a coordination game.

   1. If you want a compact description of an utterance's meaning, relativized
      to an individual speaker, use this: the update that will be made to their
      belief state when it is communicated to them. (This belief state has
      memory, and includes a distribution over all histories consistent with the
      present.)

   1. If we ignore memory, in a discrete state space this looks like a draw from
      a Dirichlet distribution.

1. Currently, most interesting semantics work looks like transforming questions
   into database queries. How do we build a question answering system that
   bottoms out in a POMDP?

   1. System zero receives an utterance, transforms it into a database query,
      executes the query, and prints the result. All mechanically, because it is
      programmed to do so.

   1. System one receives an utterance, and knows that it will receive a
      reward for returning the output of the query corresponding to the
      utterance.

   1. System two receives an utterance from a user, and knows that it will
      receive a reward for altering that user's mind to include a particular
      true belief associated with the utterance.  This is what we really want.

1. Let's generalize this:

   1. The level-zero speaker $$S_0$$ mechanically produces linguistic
      representations of its belief state.

   1. The level-zero hearer $$H_0$$ mechanically updates its belief state in
      response to language.

   1. The level-zero language-user $$L_0$$ is both $$S_0$$ and $$H_0$$. Think of this
      as a pre-theory-of-mind language-learner: whenever an event happens in the
      world, nature also produces some structured sequence of noises (a
      linguistic description of the event).

   1. The level-one speaker $$S_1$$ produces language in order to form
      appropriate beliefs in an $$H_0$$; $$H_1$$ interprets language as though
      it were produced by $$S_1$$, and so on.

   1. $$S_0$$ obeys the Gricean maxim of quality. $$S_1$$ obeys the maxim of
      quantity. $$S_2$$ obeys the maxims of relation and manner.

   1. Observation: I am $$S_2$$ and $$H_1$$ in most of my interactions with
      people, but higher levels when playing Mafia. I am not clever enough to be
      $$L_4$$.

   1. The first thing we need it a direct mapping from language to paths
      through a state space, and back again. The second thing is a good-enough
      model of other language users.

1. Does reward-chasing explain all language learning behavior?

   1. Speculation: A pre-verbal child cannot use language for anything, and 
      doesn't know that it will eventually be able to do so. Yet by the time it
      has the ability to make sounds, it has already learned the names of
      things. See also: baby sign language, Pavlov's dogs.

   1. Maybe $$L_0$$ is best explained in terms of some innate
      correlation-learning behavior ("monkey see, monkey do"), and only higher
      levels as learning to maximize rewards.

1. I can walk up to a stranger on the street, ask for the time of day, and be
   reasonably assured of receiving a correct answer.

   1. What do I mean by "ask for the time of day"? Produce a set of sounds that,
      given my model of the listener (I have a strong prior on the behavior of
      strangers on the street), will produce an appropriate update to his belief
      about my mental state.

   1. This is not the same as "produce a set of sounds that, in another speaker,
      I would interpret as the speaker's attempt to update my belief about his
      mental state". I might be in a foreign country, and have memorized the
      appropriate sequence of muscle movements, but be unable to recognize them
      when spoken to me. Symmetry is not necessary.

   1. What are my assumptions about the stranger? Easier to ask under what
      conditions this procedure fails (i.e. in which I wind up with a false
      belief about the time, or no belief at all). Possibilities: if the
      stranger doesn't the language I use, if the stranger is a sociopath, or if
      the stranger's watch is broken (nonexistent). The first two are
      interesting.

   1. How much language behavior depends on people being mostly altruistic most
      of the time? (Possibly in a stronger sense than Gricean.)

   1. A group of two adversaries won't bother to communicate
      with each other, but three will. (Think of chess for three players, or
      Diplomacy for two.) Invention of language is a Nash equilibrium here, and
      a better one than if nobody talks.

1. The interaction between language and a "database" model of the world is
   complicated.

   1. This world is basically a collection of discrete entities, each with a
      set of properties (inc. part--whole relationships, etc.).

   1. Color is one of these properties---not just real-valued, but discretized.

   1. But we know color is slightly Whorfian---my willingness to classify
      objects as _orange_ or _brown_, or _blue_ or _green_, is at least partly
      a function of my native language.

   1. Similarly, I do not notice parts of some things until they are given names
      distinct from their wholes.

   1. So the schema of this database is certainly not fixed ahead of time.

   1. But structure doesn't only flow from language to schema---also in the
      other direction. Sometimes I notice a new regularity in the world, or a
      new part. In order to incorporate it into my interior monologue, I have to
      give it a name. I do this reflexively.
